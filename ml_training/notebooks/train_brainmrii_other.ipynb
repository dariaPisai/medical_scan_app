{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d8865",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "# Import MobileNetV2 specific weights enum\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast # For Mixed Precision\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Configuration ---\n",
    "# <<< IMPORTANT: UPDATE these paths to your binary dataset location >>>\n",
    "data_base_dir = '../dataset2'\n",
    "train_dir = os.path.join(data_base_dir, 'train')\n",
    "val_dir = os.path.join(data_base_dir, 'validation')\n",
    "\n",
    "# Model parameters\n",
    "num_classes = 2 # Binary classification: brain_mri vs other_image\n",
    "batch_size = 64 # MobileNetV2 is lighter, might allow larger batch size than ResNet50\n",
    "num_epochs = 20 # Adjust as needed, monitor validation accuracy\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Image transformations parameters\n",
    "img_size = 224 # MobileNetV2 typically uses 224x224\n",
    "\n",
    "# Define the device (use GPU if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Enable cuDNN Benchmark if using GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"cuDNN Benchmark enabled.\")\n",
    "\n",
    "# Define where to save the trained binary classifier model\n",
    "binary_model_save_path = 'mobilenet_binary_classifier_weights.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f69357",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the class names for this binary task\n",
    "# IMPORTANT: These should match your folder names ('brain_mri', 'other_images')\n",
    "class_names = ['brain_mri', 'other_image']\n",
    "print(f\"Binary classification classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d21c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define transformations for the training and validation data\n",
    "# Use standard ImageNet normalization as we use a pre-trained model\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(img_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.1),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(img_size + 32), # Resize slightly larger for center crop\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "    ])\n",
    "}\n",
    "print(\"Data transforms defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bad4a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Modified Cell 4: Load Data and Split in Code ---\n",
    "\n",
    "print(f\"Loading data from base directory: {data_base_dir}\")\n",
    "print(\"Applying TRAIN transforms during initial load for splitting reference...\")\n",
    "print(\"Applying VAL transforms during second initial load for validation subset...\")\n",
    "\n",
    "\n",
    "# --- Create TWO ImageFolder datasets pointing to the SAME base directory ---\n",
    "# This is the recommended way to handle applying different transforms to train/val splits.\n",
    "# One instance uses training transforms, the other uses validation transforms.\n",
    "try:\n",
    "    # Instance applying training transforms (used for training subset)\n",
    "    full_dataset_train_transforms = datasets.ImageFolder(data_base_dir, transform=data_transforms['train'])\n",
    "\n",
    "    # Instance applying validation transforms (used for validation subset)\n",
    "    full_dataset_val_transforms = datasets.ImageFolder(data_base_dir, transform=data_transforms['val'])\n",
    "\n",
    "    print(f\"Successfully loaded dataset structure. Total images: {len(full_dataset_train_transforms)}\")\n",
    "\n",
    "    # Basic check: Ensure both instances found the same files/classes\n",
    "    if len(full_dataset_train_transforms) != len(full_dataset_val_transforms):\n",
    "         print(\"Warning: Dataset length mismatch between train/val transform instances. Check data loading.\")\n",
    "    if full_dataset_train_transforms.classes != full_dataset_val_transforms.classes:\n",
    "         print(\"Warning: Detected class mismatch between train/val transform instances.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Base dataset folder not found at {data_base_dir}.\")\n",
    "    print(\"Please ensure this path points to the directory containing 'brain_mri' and 'other_image' subfolders.\")\n",
    "    raise # Stop execution if data isn't found\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Get class names and mapping (should be same for both instances)\n",
    "detected_classes = full_dataset_train_transforms.classes\n",
    "class_to_idx = full_dataset_train_transforms.class_to_idx\n",
    "print(f\"Classes detected: {detected_classes}\")\n",
    "print(f\"Class to index mapping: {class_to_idx}\")\n",
    "\n",
    "# Verify detected classes match expected classes\n",
    "if detected_classes != class_names: # class_names defined in Cell 2\n",
    "    print(f\"Warning: Detected classes {detected_classes} do not match expected {class_names}. Check folder names.\")\n",
    "\n",
    "# --- Define the Split Ratio ---\n",
    "val_split = 0.2  # e.g., 20% for validation\n",
    "dataset_size = len(full_dataset_train_transforms)\n",
    "val_size = int(val_split * dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "print(f\"Splitting dataset: {train_size} training samples, {val_size} validation samples\")\n",
    "\n",
    "# --- Perform the Random Split ---\n",
    "# Important: Split based on indices, then create Subsets using the appropriate transform dataset\n",
    "\n",
    "# Generate indices and shuffle them\n",
    "indices = list(range(dataset_size))\n",
    "np.random.seed(42) # Optional: for reproducible splits\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[:train_size], indices[train_size:]\n",
    "\n",
    "# Create Subset datasets using the shuffled indices and the corresponding transform dataset\n",
    "train_dataset_subset = torch.utils.data.Subset(full_dataset_train_transforms, train_indices)\n",
    "val_dataset_subset = torch.utils.data.Subset(full_dataset_val_transforms, val_indices)\n",
    "\n",
    "x\n",
    "# --- Create DataLoaders from the Subset datasets ---\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset_subset, batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "    'val': DataLoader(val_dataset_subset, batch_size=batch_size, shuffle=False, num_workers=4) # No shuffle for validation\n",
    "}\n",
    "\n",
    "dataset_sizes = {'train': len(train_dataset_subset), 'val': len(val_dataset_subset)}\n",
    "print(\"DataLoaders created using random split from the base dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb9862",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNetV2 using recommended 'weights' parameter\n",
    "try:\n",
    "    model = models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT)\n",
    "    print(\"Loaded pre-trained MobileNetV2 weights (DEFAULT).\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not download pre-trained weights: {e}. Initializing random weights.\")\n",
    "    model = models.mobilenet_v2(weights=None)\n",
    "\n",
    "# MobileNetV2 has a 'classifier' layer which is a Sequential module.\n",
    "# The actual Linear layer is the last element (index 1).\n",
    "num_ftrs = model.classifier[1].in_features # Get features into the final linear layer\n",
    "\n",
    "# Replace the classifier's final layer for binary classification (num_classes = 2)\n",
    "model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Move the model to the specified device\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"MobileNetV2 model loaded and final layer modified for binary classification.\")\n",
    "# print(model) # Optional: print model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd7eebe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the loss function (CrossEntropyLoss works for 2 classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize GradScaler for Mixed Precision\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "print(\"Loss function, optimizer, and GradScaler defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77510fcf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Starting training for binary classifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Variables to track best model based on validation accuracy\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()  # Set model to training mode\n",
    "        else:\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            # Track history only in train phase for optimizer step\n",
    "            # Use autocast for mixed precision\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                with autocast(enabled=(device.type == 'cuda')): # Enable AMP only on GPU\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1) # Get predictions\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        if torch.isnan(loss):\n",
    "                            print(f\"WARNING: NaN loss detected at epoch {epoch+1}, batch {i+1} ({phase}). Skipping batch.\")\n",
    "                            continue\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # Print progress periodically during training\n",
    "            # if phase == 'train' and (i + 1) % 100 == 0:\n",
    "            #      print(f'  Batch {i+1}/{len(dataloaders[phase])}, Current Batch Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "        print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        # Deep copy the model if validation accuracy improves\n",
    "        if phase == 'val' and epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            print(f'  -> Best Validation Acc: {best_acc:.4f} (saved model weights)')\n",
    "\n",
    "# --- Training Complete ---\n",
    "time_elapsed = time.time() - start_time\n",
    "print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "print(f'Best Validation Accuracy: {best_acc:.4f}')\n",
    "\n",
    "# Load best model weights before saving\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d3b1fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save the state dictionary of the best model\n",
    "torch.save(model.state_dict(), binary_model_save_path)\n",
    "print(f\"Best binary classifier model weights saved to {binary_model_save_path}\")\n",
    "# This is the file you'll load in your Flask app for the initial check."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
